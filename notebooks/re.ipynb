{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class BoxPackingEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom Gym environment for the box packing problem.\n",
    "  \"\"\"\n",
    "  def __init__(self, predefined_boxes, max_item_rotations):\n",
    "    self.predefined_boxes = predefined_boxes  # List of dictionaries (dimensions and capacity)\n",
    "    self.max_item_rotations = max_item_rotations  # Max allowed rotations per item\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    # Reset the environment for a new order\n",
    "    self.remaining_items = []  # List of dictionaries (item dimensions and quantity)\n",
    "    # ... (logic to populate remaining_items based on your data generation)\n",
    "    self.available_boxes = list(self.predefined_boxes)  # Copy of predefined boxes\n",
    "    self.wasted_space = 0  # Initialize wasted space\n",
    "    self.current_item_idx = 0  # Index of the item being placed\n",
    "    self.current_item_rotations = 0  # Number of rotations performed on current item\n",
    "    return self._get_observation()\n",
    "\n",
    "  def step(self, action):\n",
    "    # Take an action and return next state, reward, done, and info\n",
    "    reward = 0\n",
    "    done = False\n",
    "    info = {}\n",
    "\n",
    "    if action == 0:  # Skip to next item (penalty)\n",
    "      reward -= 1.0\n",
    "      self.current_item_idx += 1\n",
    "      self.current_item_rotations = 0\n",
    "    else:\n",
    "      # Validate and process box selection action\n",
    "      box_idx = action - 1  # Subtract 1 to account for skip action\n",
    "      if self._box_fits_item(box_idx):\n",
    "        self._place_item_in_box(box_idx)\n",
    "        reward += 1.0  # Reward for placing item\n",
    "        reward += self._calculate_wasted_space_reward(box_idx)  # Reward for wasted space\n",
    "        self.current_item_idx += 1\n",
    "        self.current_item_rotations = 0\n",
    "      else:\n",
    "        reward -= 0.5  # Penalty for trying an unfit box\n",
    "\n",
    "    # Check if all items packed or no more options\n",
    "    done = self.current_item_idx >= len(self.remaining_items) or self._all_boxes_full()\n",
    "\n",
    "    # Check for exceeding box capacity (large penalty)\n",
    "    if self._any_box_overfilled():\n",
    "      done = True\n",
    "      reward -= 10.0  # Large penalty for exceeding capacity\n",
    "\n",
    "    return self._get_observation(), reward, done, info\n",
    "\n",
    "  def _get_observation(self):\n",
    "    # Convert state information to a suitable representation for the RL agent (refer to Response 3)\n",
    "    state_rep = self.state.get_state_representation()\n",
    "    return state_rep\n",
    "\n",
    "  def _box_fits_item(self, box_idx):\n",
    "    # Implement logic to check if the current item fits in the chosen box after rotations\n",
    "    # Consider all possible rotations (up to max_item_rotations) of the item\n",
    "    for _ in range(self.max_item_rotations + 1):\n",
    "      # ... (logic to check if item fits in the box with current rotations)\n",
    "      if item_fits_in_box:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def _place_item_in_box(self, box_idx):\n",
    "    # Update state information after placing the item in the chosen box\n",
    "    # Update box capacity and wasted space\n",
    "    self.wasted_space += self._calculate_wasted_space_in_box(box_idx)\n",
    "    # ... (logic to update box capacity and remaining items)\n",
    "\n",
    "  def _calculate_wasted_space_reward(self, box_idx):\n",
    "    # Calculate reward based on remaining volume in the chosen box\n",
    "    # ... (logic to calculate wasted space reward based on box capacity and remaining volume)\n",
    "    return wasted_space_reward\n",
    "\n",
    "  def _any_box_overfilled(self):\n",
    "    # Check if any box has exceeded its capacity\n",
    "    for box in self.available_boxes:\n",
    "      if box[\"remaining_capacity\"] < 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def _all_boxes_full(self):\n",
    "    # Check if all available boxes are full\n",
    "    for box in self.available_boxes:\n",
    "      if box[\"remaining_capacity\"] > 0:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "  def __init__(self, remaining_items, available_boxes):\n",
    "    self.remaining_items = remaining_items  # List of dictionaries (item dimensions and quantity)\n",
    "    self.available_boxes = available_boxes  # List of dictionaries (box dimensions and capacity remaining)\n",
    "    self.wasted_space = 0  # Placeholder, needs calculation based on specific use case\n",
    "\n",
    "  def get_state_representation(self):\n",
    "    # This function converts the state into a suitable representation for the RL agent\n",
    "    # Refer to Response 3 for a breakdown of this function\n",
    "    pass\n",
    "\n",
    "  def get_total_volume(self):\n",
    "    # Calculate the total volume of all remaining items\n",
    "    total_volume = 0\n",
    "    for item in self.remaining_items:\n",
    "      item_volume = item[\"width\"] * item[\"height\"] * item[\"depth\"] * item[\"quantity\"]\n",
    "      total_volume += item_volume\n",
    "    return total_volume\n",
    "\n",
    "\n",
    "def calculate_wasted_space_in_box(box):\n",
    "  # Calculate wasted space within a box based on your chosen approach\n",
    "  # This example calculates wasted space as a ratio of remaining volume to total capacity\n",
    "  wasted_space_ratio = box[\"remaining_capacity\"] / box[\"total_capacity\"]\n",
    "  return wasted_space_ratio * box[\"total_capacity\"]  # Convert ratio to volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "class DQN(tf.keras.Model):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super(DQN, self).__init__()\n",
    "    self.conv1 = Conv2D(32, (3, 3), activation='relu', input_shape=(state_size[0], state_size[1], 1))\n",
    "    self.flatten = Flatten()\n",
    "    self.fc1 = Dense(64, activation='relu')\n",
    "    self.fc2 = Dense(action_size)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc1(x)\n",
    "    return self.fc2(x)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  def __init__(self, max_size, input_shape, action_size):\n",
    "    self.buffer_size = max_size\n",
    "    self.count = 0\n",
    "    self.state_buffer = np.zeros((max_size, *input_shape))\n",
    "    self.action_buffer = np.zeros(max_size)\n",
    "    self.reward_buffer = np.zeros(max_size)\n",
    "    self.next_state_buffer = np.zeros((max_size, *input_shape))\n",
    "    self.done_buffer = np.zeros(max_size)\n",
    "\n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    index = self.count % self.buffer_size\n",
    "    self.state_buffer[index] = state\n",
    "    self.action_buffer[index] = action\n",
    "    self.reward_buffer[index] = reward\n",
    "    self.next_state_buffer[index] = next_state\n",
    "    self.done_buffer[index] = done\n",
    "    self.count += 1\n",
    "\n",
    "  def size(self):\n",
    "    return self.count % self.buffer_size\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    max_buffer_size = min(self.count, self.buffer_size)\n",
    "    indices = np.random.choice(max_buffer_size, size=batch_size, replace=False)\n",
    "    return (\n",
    "      self.state_buffer[indices],\n",
    "      self.action_buffer[indices],\n",
    "      self.reward_buffer[indices],\n",
    "      self.next_state_buffer[indices],\n",
    "      self.done_buffer[indices])\n",
    "\n",
    "def train_dqn(env, agent, replay_buffer, num_episodes, batch_size, gamma, learning_rate):\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, (1, *state.shape))  # Reshape for CNN input\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      action = np.argmax(agent(state))\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      next_state = np.reshape(next_state, (1, *next_state.shape))  # Reshape for CNN input\n",
    "      episode_reward += reward\n",
    "\n",
    "      replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "      state = next_state\n",
    "\n",
    "      if replay_buffer.size() >= batch_size:\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        q_values = agent(states)\n",
    "        next_q_values = agent(next_states)\n",
    "        q_target = rewards + gamma * tf.reduce_max(next_q_values, axis=1, keepdims=True) * (1 - dones)\n",
    "\n",
    "        # Update critic network using Huber loss (less sensitive to outliers)\n",
    "        with tf.GradientTape() as tape:\n",
    "          q_value = q_values[0, actions]\n",
    "          loss = tf.losses.Huber()(q_target, q_value)\n",
    "        grads = tape.gradient(loss, agent.trainable_variables)\n",
    "        optimizer.apply_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "\n",
    "class DQN(tf.keras.Model):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super(DQN, self).__init__()\n",
    "    self.conv1 = Conv2D(32, (3, 3), activation='relu', input_shape=(state_size[0], state_size[1], 1))\n",
    "    self.flatten = Flatten()\n",
    "    self.fc1 = Dense(64, activation='relu')\n",
    "    self.fc2 = Dense(action_size)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc1(x)\n",
    "    return self.fc2(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  def __init__(self, max_size, input_shape, action_size):\n",
    "    self.buffer_size = max_size\n",
    "    self.count = 0\n",
    "    self.state_buffer = np.zeros((max_size, *input_shape))\n",
    "    self.action_buffer = np.zeros(max_size)\n",
    "    self.reward_buffer = np.zeros(max_size)\n",
    "    self.next_state_buffer = np.zeros((max_size, *input_shape))\n",
    "    self.done_buffer = np.zeros(max_size)\n",
    "\n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    index = self.count % self.buffer_size\n",
    "    self.state_buffer[index] = state\n",
    "    self.action_buffer[index] = action\n",
    "    self.reward_buffer[index] = reward\n",
    "    self.next_state_buffer[index] = next_state\n",
    "    self.done_buffer[index] = done\n",
    "    self.count += 1\n",
    "\n",
    "  def size(self):\n",
    "    return self.count % self.buffer_size\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    max_buffer_size = min(self.count, self.buffer_size)\n",
    "    indices = np.random.choice(max_buffer_size, size=batch_size, replace=False)\n",
    "    return (\n",
    "      self.state_buffer[indices],\n",
    "      self.action_buffer[indices],\n",
    "      self.reward_buffer[indices],\n",
    "      self.next_state_buffer[indices],\n",
    "      self.done_buffer[indices])\n",
    "\n",
    "\n",
    "def train_dqn(env, agent, replay_buffer, num_episodes, batch_size, gamma, learning_rate, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "  \"\"\"\n",
    "  Advanced DQN training function with epsilon-greedy exploration and training metrics tracking.\n",
    "  \"\"\"\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  episode_rewards = []  # Track episode rewards for monitoring\n",
    "\n",
    "  for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, (1, *state.shape))  # Reshape for CNN input\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      # Epsilon-greedy action selection\n",
    "      if np.random.rand() < epsilon:\n",
    "        action = env.action_space.sample()  # Explore randomly\n",
    "      else:\n",
    "        q_values = agent(state)\n",
    "        action = np.argmax(q_values)  # Exploit based on Q-values\n",
    "\n",
    "      # Take action in the environment\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      next_state = np.reshape(next_state, (1, *next_state.shape))  # Reshape for CNN input\n",
    "      episode_reward += reward\n",
    "\n",
    "      replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "      state = next_state\n",
    "\n",
    "      # Train DQN using experience replay when enough samples are available\n",
    "      if replay_buffer.size() >= batch_size:\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        q_values\n",
    "        q_values = agent(states)\n",
    "        next_q_values = agent(next_states)  # Calculate Q-values for next states using the DQN model\n",
    "\n",
    "        # Calculate target Q-values (q_target) using Bellman equation\n",
    "        q_target = rewards + gamma * tf.reduce_max(next_q_values, axis=1, keepdims=True) * (1 - dones)\n",
    "\n",
    "        # Update critic network using Huber loss\n",
    "        with tf.GradientTape() as tape:\n",
    "          q_value = q_values[0, actions]\n",
    "          loss = tf.losses.Huber()(q_target, q_value)\n",
    "        grads = tape.gradient(loss, agent.trainable_variables)\n",
    "        optimizer.apply_gradients(grads)\n",
    "\n",
    "      # Update epsilon for epsilon-greedy exploration (gradually decrease exploration)\n",
    "      epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    # Print episode statistics (optional)\n",
    "    if episode % 100 == 0:\n",
    "      print(f\"Episode: {episode}, Average Reward: {np.mean(episode_rewards[-100:])}\")\n",
    "\n",
    "  return episode_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class BoxPackingEnv(gym.Env):\n",
    "  \"\"\"\n",
    "  Custom environment for box packing with DQN agent.\n",
    "  \"\"\"\n",
    "  def __init__(self, box_sizes, max_items):\n",
    "    \"\"\"\n",
    "    Initialize the environment with available box sizes and maximum items per order.\n",
    "\n",
    "    Args:\n",
    "        box_sizes (list): List of tuples representing available box dimensions (length, width, height).\n",
    "        max_items (int): Maximum number of items in an order.\n",
    "    \"\"\"\n",
    "    self.box_sizes = box_sizes  # List of (length, width, height) tuples\n",
    "    self.max_items = max_items\n",
    "    self.reset()  # Reset for initial state\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    Reset the environment to a new packing scenario.\n",
    "\n",
    "    Returns:\n",
    "        observation (numpy.ndarray): Initial state representation.\n",
    "    \"\"\"\n",
    "    self.remaining_items = []  # List of dictionaries (item_id, length, width, height)\n",
    "    self.used_boxes = []  # List of chosen box dimensions during packing\n",
    "    self.current_box = None  # Currently selected box (length, width, height)\n",
    "    self.reward = 0  # Cumulative reward for packing\n",
    "\n",
    "    # Generate random order data\n",
    "    num_items = np.random.randint(1, self.max_items + 1)\n",
    "    for _ in range(num_items):\n",
    "      item_id = len(self.remaining_items)\n",
    "      length, width, height = np.random.uniform(0.1, 1.0, size=3)  # Random item dimensions\n",
    "      self.remaining_items.append({\"item_id\": item_id, \"length\": length, \"width\": width, \"height\": height})\n",
    "\n",
    "    # Initial state representation (remaining items count, available box sizes)\n",
    "    state = np.array([len(self.remaining_items)] + [0] * len(self.box_sizes))\n",
    "    for i, box_size in enumerate(self.box_sizes):\n",
    "      state[1 + i] = 1 if self.can_fit_box(box_size) else 0  # Indicate if box can fit remaining items\n",
    "\n",
    "    return state\n",
    "\n",
    "  def step(self, action):\n",
    "    \"\"\"\n",
    "    Take an action (box selection) and update the environment.\n",
    "\n",
    "    Args:\n",
    "        action (int): Index of the chosen box size from available options.\n",
    "\n",
    "    Returns:\n",
    "        observation (numpy.ndarray): Updated state representation.\n",
    "        reward (float): Reward for the action.\n",
    "        done (bool): Whether the episode is finished (all items packed or no valid boxes).\n",
    "        info (dict): Additional information (optional).\n",
    "    \"\"\"\n",
    "    if self.current_box is not None:  # If a box is already selected, rotate the item\n",
    "      self.reward -= 0.1  # Penalize rotation as it's generally less space-efficient\n",
    "      return self._rotate_item(action)\n",
    "\n",
    "    # Select the chosen box size\n",
    "    box_size = self.box_sizes[action]\n",
    "    self.current_box = box_size\n",
    "\n",
    "    # Check if all items can fit in the chosen box\n",
    "    if self.can_fit_box(box_size):\n",
    "      reward = self._calculate_reward(box_size)\n",
    "      self.reward += reward\n",
    "      self.used_boxes.append(box_size)\n",
    "      for item in self.remaining_items:\n",
    "        self.remaining_items.remove(item)  # Remove packed items\n",
    "      self.current_box = None  # Clear current box selection\n",
    "\n",
    "      # Check if all items are packed or no valid boxes are left\n",
    "      done = len(self.remaining_items) == 0 or all(not self.can_fit_box(box_size) for box_size in self.box_sizes)\n",
    "    else:\n",
    "      reward = -1  # Penalty for trying a box that doesn't fit\n",
    "      done = True  # Episode terminated due to no fitting box\n",
    "\n",
    "    # Update state representation\n",
    "    state = np.array([len(self.remaining_items)] + [0] * len(self.box_sizes))\n",
    "    for i, box_size in enumerate(self.box_sizes):\n",
    "      state[1 + i] = 1 if self.can_fit_box(box_size) else 0\n",
    "\n",
    "    return state, reward, done, {}  # Empty info dictionary for now\n",
    "\n",
    "  def _can_fit_item(self, item, box):\n",
    "    \"\"\"\n",
    "    Checks if a single item can fit inside a given box in any orientation.\n",
    "\n",
    "    Args:\n",
    "        item (dict): Dictionary containing item dimensions (length, width, height).\n",
    "        box (tuple): Tuple representing box dimensions (length, width, height).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the item can fit in the box, False otherwise.\n",
    "    \"\"\"\n",
    "    for _ in range(6):  # Check all 6 possible item rotations\n",
    "      if (item[\"length\"] <= box[0] and item[\"width\"] <= box[1] and item[\"height\"] <= box[2]) or \\\n",
    "         (item[\"length\"] <= box[1] and item[\"width\"] <= box[0] and item[\"height\"] <= box[2]) or \\\n",
    "         (item[\"length\"] <= box[0] and item[\"width\"] <= box[2] and item[\"height\"] <= box[1]) or \\\n",
    "         (item[\"length\"] <= box[1] and item[\"width\"] <= box[2] and item[\"height\"] <= box[0]) or \\\n",
    "         (item[\"length\"] <= box[2] and item[\"width\"] <= box[0] and item[\"height\"] <= box[1]) or \\\n",
    "         (item[\"length\"] <= box[2] and item[\"width\"] <= box[1] and item[\"height\"] <= box[0]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "  def can_fit_box(self, box_size):\n",
    "    \"\"\"\n",
    "    Checks if all remaining items can fit inside a given box.\n",
    "\n",
    "    Args:\n",
    "        box_size (tuple): Tuple representing box dimensions (length, width, height).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all items can fit in the box, False otherwise.\n",
    "    \"\"\"\n",
    "    return all(self._can_fit_item(item, box_size) for item in self.remaining_items)\n",
    "\n",
    "  def _calculate_reward(self, box_size):\n",
    "    \"\"\"\n",
    "    Calculates the reward for using a specific box size.\n",
    "\n",
    "    Args:\n",
    "        box_size (tuple): Tuple representing box dimensions (length, width, height).\n",
    "\n",
    "    Returns:\n",
    "        float: Reward value for using the box.\n",
    "    \"\"\"\n",
    "    volume_used = np.prod(self.current_box)\n",
    "    total_item_volume = sum(item[\"length\"] * item[\"width\"] * item[\"height\"] for item in self.remaining_items)\n",
    "    wasted_space = max(0, volume_used - total_item_volume)\n",
    "    reward = volume_used / (total_item_volume + 1) - wasted_space * 0.1 - 0.05  # Adjust penalty coefficients\n",
    "\n",
    "    return reward\n",
    "\n",
    "  def _rotate_item(self, action):\n",
    "    \"\"\"\n",
    "    Simulates rotating the currently selected item within the box.\n",
    "\n",
    "    Args:\n",
    "        action (int): Index representing the rotation direction (e.g., 0: no rotation, 1: rotate 90 degrees).\n",
    "\n",
    "    Returns:\n",
    "        observation (numpy.ndarray): Updated state representation.\n",
    "        reward (float): Reward for the rotation (usually negative due to inefficiency).\n",
    "        done (bool): Whether the episode is finished (all items packed or no valid rotations).\n",
    "        info (dict): Additional information (optional).\n",
    "    \"\"\"\n",
    "    # Implement your item rotation logic here.\n",
    "    # Update state and reward based on the rotation outcome (successful or not).\n",
    "    # Episode might terminate if no valid rotations are possible.\n",
    "\n",
    "    # Placeholder logic for demonstration (no actual rotation)\n",
    "    possible_rotations = 3  # Replace with the number of supported rotations (e.g., 0, 90, 180 degrees)\n",
    "    if action < possible_rotations:\n",
    "      self.reward -= 0.02  # Small penalty for attempting rotation\n",
    "    else:\n",
    "      done = True  # No more rotations possible, terminate episode\n",
    "\n",
    "    return self._get_observation(), self.reward, done, {}\n",
    "\n",
    "  def _get_observation(self):\n",
    "    \"\"\"\n",
    "    Returns the current state representation of the environment.\n",
    "    \"\"\"\n",
    "    state = np.array([len(self.remaining_items)] + [0] * len(self.box_sizes))\n",
    "    for i, box_size in enumerate(self.box_sizes):\n",
    "      state[1 + i] = 1 if self.can_fit_box(box_size) else 0\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DQNAgent(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  Deep Q-Network agent for box packing.\n",
    "  \"\"\"\n",
    "  def __init__(self, state_size, action_size):\n",
    "    \"\"\"\n",
    "    Initialize the DQN agent with state and action space sizes.\n",
    "\n",
    "    Args:\n",
    "        state_size (int): Dimensionality of the environment state.\n",
    "        action_size (int): Number of available actions (box sizes).\n",
    "    \"\"\"\n",
    "    super(DQNAgent, self).__init__()\n",
    "    self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(state_size[0], state_size[1], 1))\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "    self.fc1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "    self.fc2 = tf.keras.layers.Dense(action_size)\n",
    "\n",
    "  def call(self, state):\n",
    "    \"\"\"\n",
    "    Forward pass of the DQN model to predict Q-values.\n",
    "\n",
    "    Args:\n",
    "        state (tf.Tensor): Input state representation.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Predicted Q-values for all actions.\n",
    "    \"\"\"\n",
    "    x = tf.expand_dims(state, axis=-1)  # Add channel dimension for CNN\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.fc1(x)\n",
    "    q_values = self.fc2(x)\n",
    "    return q_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  \"\"\"\n",
    "  Replay buffer for storing past experiences for training.\n",
    "  \"\"\"\n",
    "  def __init__(self, capacity):\n",
    "    \"\"\"\n",
    "    Initialize the replay buffer with a fixed capacity.\n",
    "\n",
    "    Args:\n",
    "        capacity (int): Maximum number of experiences to store.\n",
    "    \"\"\"\n",
    "    self.buffer = []\n",
    "    self.capacity = capacity\n",
    "    self.position = 0\n",
    "\n",
    "  def add(self, experience):\n",
    "    \"\"\"\n",
    "    Add a new experience (state, action, reward, next_state, done) to the buffer.\n",
    "\n",
    "    Args:\n",
    "        experience (tuple): Tuple containing the experience data.\n",
    "    \"\"\"\n",
    "    if self.position == self.capacity:\n",
    "      # Replace the oldest experience if buffer is full\n",
    "      self.buffer[self.position] = experience\n",
    "    else:\n",
    "      self.buffer.append(experience)\n",
    "      self.position += 1\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    \"\"\"\n",
    "    Sample a random batch of experiences from the replay buffer.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Size of the desired experience batch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of sampled states, actions, rewards, next states, and done flags.\n",
    "    \"\"\"\n",
    "    experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    states, actions, rewards, next_states, done = zip(*experiences)\n",
    "    return states, actions, rewards, next_states, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "def train_dqn(env, agent, replay_buffer, num_episodes, batch_size, gamma, learning_rate):\n",
    "    \"\"\"\n",
    "    Train the DQN agent on the box packing environment.\n",
    "\n",
    "    Args:\n",
    "        env (BoxPackingEnv): Box packing environment instance.\n",
    "        agent (DQNAgent): DQN agent model.\n",
    "        replay_buffer (ReplayBuffer): Replay buffer for storing experiences.\n",
    "        num_episodes (int): Number of training episodes.\n",
    "        batch_size (int): Batch size for experience replay.\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "    \"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = HuberLoss()  # Consider using Huber loss for robustness\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Select action based on epsilon-greedy policy\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Step through the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.add((state, action, reward, next_state, done))\n",
    "\n",
    "            # Sample a batch of experiences for training\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, done_flags = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Calculate Q-value targets for training\n",
    "                q_values = agent(states)\n",
    "                next_q_values = agent(next_states)\n",
    "                q_value_targets = rewards + gamma * tf.math.reduce_max(next_q_values, axis=1, keepdims=True) * (1 - done_flags)\n",
    "\n",
    "                # Train the DQN agent with Huber loss\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predicted_q_values = agent(states)\n",
    "                    loss = loss_fn(q_value_targets, predicted_q_values)\n",
    "                grads = tape.gradient(loss, agent.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, agent.trainable_variables))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Print episode progress (optional)\n",
    "        print(f\"Episode: {episode+1}/{num_episodes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
